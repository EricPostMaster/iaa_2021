{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sparkml_simple_pipeline.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-7vRh7EGS30"
      },
      "source": [
        "## **PySpark Machine Learning**\n",
        "###Pipeline Example ([Docs](https://spark.apache.org/docs/latest/ml-pipeline.html))\n",
        "\n",
        "In machine learning, it's common to run a series of steps for data prep, cleansing, feature engineering, and then ultimately model training (among several other potential steps). \n",
        "\n",
        "Spark ML Pipelines sequences these steps into an ordered array (or DAG). A Pipeline is specified as a sequence of stages, and each stage is either a **Transformer** or an **Estimator**.\n",
        "\n",
        "It's often a best practice to save a model or a pipeline to disk for later use.\n",
        "\n",
        "Below is an example Spark ML Pipeline that shows two Transformers (Tokenizer and HashingTF) and one Estimator (Logistic Regression). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcHwihQDCpaf"
      },
      "source": [
        "<img src=\"https://spark.apache.org/docs/latest/img/ml-Pipeline.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ24BRz6FMwN"
      },
      "source": [
        "## **Install Spark Dependencies**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upqpisH2IoMy"
      },
      "source": [
        "# Install Spark dependencies\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!rm spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!wget --no-cookies --no-check-certificate https://ftp.wayne.edu/apache/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar zxvf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2xvv-QnsQZs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f05ccde-4d7d-446a-a057-1a74ee2ee98d"
      },
      "source": [
        "!ls -al | grep spark"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drwxr-xr-x 13 1000 1000      4096 Feb 22 02:11 spark-3.1.1-bin-hadoop3.2\n",
            "-rw-r--r--  1 root root 228721937 Feb 22 02:45 spark-3.1.1-bin-hadoop3.2.tgz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvMtRJwUFzie"
      },
      "source": [
        "## **Import Python and PySpark Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-SIpC_-aw0t"
      },
      "source": [
        "# Set up required environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"]  = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import HashingTF, Tokenizer"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyT917EuF7Pv"
      },
      "source": [
        "## **Initialize Spark Session**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niAz2S672M_m"
      },
      "source": [
        "spark = SparkSession.builder.appName(\"Spark ML Pipeline Example\").master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL4NPf_ZF_OF"
      },
      "source": [
        "## **Load Sample Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udgQ-I48MRrV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b56eb6e-d2d8-4336-c0f9-b80f3f4ffe7f"
      },
      "source": [
        "training = spark.createDataFrame([\n",
        "    (0, \"a b c d e spark\", 1.0),\n",
        "    (1, \"b d\", 0.0),\n",
        "    (2, \"spark f g h\", 1.0),\n",
        "    (3, \"hadoop mapreduce\", 0.0)\n",
        "], [\"id\", \"text\", \"label\"])\n",
        "\n",
        "training.show(10,False)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+----------------+-----+\n",
            "|id |text            |label|\n",
            "+---+----------------+-----+\n",
            "|0  |a b c d e spark |1.0  |\n",
            "|1  |b d             |0.0  |\n",
            "|2  |spark f g h     |1.0  |\n",
            "|3  |hadoop mapreduce|0.0  |\n",
            "+---+----------------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9qcdXwEMZqY"
      },
      "source": [
        "## **Configure Pipeline Objects**\n",
        "Transforms (tokenizer and hashingTF) and Estimators (logistic regression)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4WCZ13oMltr"
      },
      "source": [
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
        "lr = LogisticRegression(maxIter=10, regParam=0.001)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kT_dTa7LpkdK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e10e750-30a9-4cb9-ab17-28b1825d893b"
      },
      "source": [
        "tokenizer.transform(training).show(5, False)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+----------------+-----+----------------------+\n",
            "|id |text            |label|words                 |\n",
            "+---+----------------+-----+----------------------+\n",
            "|0  |a b c d e spark |1.0  |[a, b, c, d, e, spark]|\n",
            "|1  |b d             |0.0  |[b, d]                |\n",
            "|2  |spark f g h     |1.0  |[spark, f, g, h]      |\n",
            "|3  |hadoop mapreduce|0.0  |[hadoop, mapreduce]   |\n",
            "+---+----------------+-----+----------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLnHzeSgMsIX"
      },
      "source": [
        "## **Create Pipeline Object**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4ip4waqMwLd"
      },
      "source": [
        "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgaJUUnVMy10"
      },
      "source": [
        "## **Run Pipeline to transform data and train model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uotehx7bM42W"
      },
      "source": [
        "model = pipeline.fit(training)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPWvt6emNzaz"
      },
      "source": [
        "#dir(model.stages[-1].summary)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYuCIf6tM8ot"
      },
      "source": [
        "## **Test Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hC6C3wKwyBbt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f267e957-497b-458a-efb6-62d1609413a2"
      },
      "source": [
        "test = spark.createDataFrame([\n",
        "    (4, \"spark i j k\"),\n",
        "    (5, \"l m n\"),\n",
        "    (6, \"spark hadoop spark\"),\n",
        "    (7, \"apache hadoop\")\n",
        "], [\"id\", \"text\"])\n",
        "\n",
        "# Make predictions on test documents and print columns of interest.\n",
        "prediction = model.transform(test)\n",
        "prediction.show(10,False)\n",
        "#selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
        "#for row in selected.collect():\n",
        "#    rid, text, prob, prediction = row\n",
        "#    print(\"(%d, %s) --> prob=%s, prediction=%f\" % (rid, text, str(prob), prediction))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------------------+----------------------+------------------------------------------------------+----------------------------------------+-----------------------------------------+----------+\n",
            "|id |text              |words                 |features                                              |rawPrediction                           |probability                              |prediction|\n",
            "+---+------------------+----------------------+------------------------------------------------------+----------------------------------------+-----------------------------------------+----------+\n",
            "|4  |spark i j k       |[spark, i, j, k]      |(262144,[19036,68693,173558,213660],[1.0,1.0,1.0,1.0])|[-1.660903322747327,1.660903322747327]  |[0.1596407738787412,0.8403592261212588]  |1.0       |\n",
            "|5  |l m n             |[l, m, n]             |(262144,[1303,52644,248090],[1.0,1.0,1.0])            |[1.6421889526563522,-1.6421889526563522]|[0.8378325685476614,0.16216743145233858] |0.0       |\n",
            "|6  |spark hadoop spark|[spark, hadoop, spark]|(262144,[173558,198017],[2.0,1.0])                    |[-2.5980142174392933,2.5980142174392933]|[0.06926633132976266,0.9307336686702373] |1.0       |\n",
            "|7  |apache hadoop     |[apache, hadoop]      |(262144,[68303,198017],[1.0,1.0])                     |[4.008170333368065,-4.008170333368065]  |[0.9821575333444208,0.017842466655579203]|0.0       |\n",
            "+---+------------------+----------------------+------------------------------------------------------+----------------------------------------+-----------------------------------------+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3odXGl1rgrg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}